@article{Drozdov2020,
   abstract = {Chest radiography (CXR) is the most commonly used imaging modality and deep neural network (DNN) algorithms have shown promise in effective triage of normal and abnormal radiograms. Typically, DNNs require large quantities of expertly labelled training exemplars, which in clinical contexts is a major bottleneck to effective modelling, as both considerable clinical skill and time is required to produce high-quality ground truths. In this work we evaluate thirteen supervised classifiers using two large free-text corpora and demonstrate that bidirectional long short-term memory (BiLSTM) networks with attention mechanism effectively identify Normal, Abnormal, and Unclear CXR reports in internal (n = 965 manually-labelled reports, f1-score = 0.94) and external (n = 465 manually-labelled reports, f1-score = 0.90) testing sets using a relatively small number of expert-labelled training observations (n = 3,856 annotated reports). Furthermore, we introduce a general unsupervised approach that accurately distinguishes Normal and Abnormal CXR reports in a large unlabelled corpus. We anticipate that the results presented in this work can be used to automatically extract standardized clinical information from free-text CXR radiological reports, facilitating the training of clinical decision support systems for CXR triage.},
   author = {Ignat Drozdov and Daniel Forbes and Benjamin Szubert and Mark Hall and Chris Carlin and David J. Lowe},
   doi = {10.1371/journal.pone.0229963},
   issn = {19326203},
   issue = {3},
   journal = {PLoS ONE},
   pmid = {32155219},
   publisher = {Public Library of Science},
   title = {Supervised and unsupervised language modelling in Chest X-Ray radiological reports},
   volume = {15},
   year = {2020},
}
@article{Bazi2023,
   abstract = {<p>In the clinical and healthcare domains, medical images play a critical role. A mature medical visual question answering system (VQA) can improve diagnosis by answering clinical questions presented with a medical image. Despite its enormous potential in the healthcare industry and services, this technology is still in its infancy and is far from practical use. This paper introduces an approach based on a transformer encoder–decoder architecture. Specifically, we extract image features using the vision transformer (ViT) model, and we embed the question using a textual encoder transformer. Then, we concatenate the resulting visual and textual representations and feed them into a multi-modal decoder for generating the answer in an autoregressive way. In the experiments, we validate the proposed model on two VQA datasets for radiology images termed VQA-RAD and PathVQA. The model shows promising results compared to existing solutions. It yields closed and open accuracies of 84.99% and 72.97%, respectively, for VQA-RAD, and 83.86% and 62.37%, respectively, for PathVQA. Other metrics such as the BLUE score showing the alignment between the predicted and true answer sentences are also reported.</p>},
   author = {Yakoub Bazi and Mohamad Mahmoud Al Rahhal and Laila Bashmal and Mansour Zuair},
   doi = {10.3390/bioengineering10030380},
   issn = {2306-5354},
   issue = {3},
   journal = {Bioengineering},
   month = {3},
   pages = {380},
   title = {Vision–Language Model for Visual Question Answering in Medical Imagery},
   volume = {10},
   url = {https://www.mdpi.com/2306-5354/10/3/380},
   year = {2023},
}
@inproceedings{Barnes2016,
   abstract = {The purpose of the paper is to discuss human centered design implications for shared decision making between humans and autonomous systems in complex environments. Design implications are generated based on empirical results from two research paradigms. In the first paradigm, an intelligent agent (Robo Leader) supervised multiple subordinate systems and was in turn supervised by the human operator. The Robo Leader research varied number of subordinate units, task difficulty, agent reliability, type of agent errors, and partial autonomy. The second paradigm involved human interaction with partially and fully autonomous systems. Design implications from both paradigms are evaluated-relating to multitasking, adaptive systems, false alarms, individual differences, operator trust, and allocation of human and agent tasks for partial autonomy. We conclude that mixed-initiative decision sharing depends on designing interfaces that support human-Agent transparency.},
   author = {Michael J. Barnes and Jessie Y.C. Chen and Florian Jentsch},
   doi = {10.1109/SMC.2015.246},
   isbn = {9781479986965},
   journal = {Proceedings - 2015 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2015},
   keywords = {autonomous agents,human-centered design,mixed-initiative decision making},
   month = {1},
   pages = {1386-1390},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Designing for Mixed-Initiative Interactions between Human and Autonomous Systems in Complex Environments},
   year = {2016},
}
@article{Madani2020,
   abstract = {Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on ~280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy.},
   author = {Ali Madani and Bryan McCann and Nikhil Naik and Nitish Shirish Keskar and Namrata Anand and Raphael R. Eguchi and Po-Ssu Huang and Richard Socher},
   month = {3},
   title = {ProGen: Language Modeling for Protein Generation},
   url = {http://arxiv.org/abs/2004.03497},
   year = {2020},
}
@article{Li2023,
   abstract = {Recent large language models (LLMs) in the general domain, such as ChatGPT, have shown remarkable success in following instructions and producing human-like responses. However, such language models have not been tailored to the medical domain, resulting in poor answer accuracy and inability to give plausible recommendations for medical diagnosis, medications, etc. To address this issue, we collected more than 700 diseases and their corresponding symptoms, required medical tests, and recommended medications, from which we generated 5K doctor-patient conversations. In addition, we obtained 200K real patient-doctor conversations from online Q\&A medical consultation sites. By fine-tuning LLMs using these 205k doctor-patient conversations, the resulting models emerge with great potential to understand patients' needs, provide informed advice, and offer valuable assistance in a variety of medical-related fields. The integration of these advanced language models into healthcare can revolutionize the way healthcare professionals and patients communicate, ultimately improving the overall efficiency and quality of patient care and outcomes. In addition, we made public all the source codes, datasets, and model weights to facilitate the further development of dialogue models in the medical field. The training data, codes, and weights of this project are available at: The training data, codes, and weights of this project are available at: https://github.com/Kent0n-Li/ChatDoctor.},
   author = {Yunxiang Li and Zihan Li and Kai Zhang and Ruilong Dan and You Zhang},
   month = {3},
   title = {ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge},
   url = {http://arxiv.org/abs/2303.14070},
   year = {2023},
}
@article{Wang2023,
   abstract = {Large language models (LLMs) have recently demonstrated their potential in clinical applications, providing valuable medical knowledge and advice. For example, a large dialog LLM like ChatGPT has successfully passed part of the US medical licensing exam. However, LLMs currently have difficulty processing images, making it challenging to interpret information from medical images, which are rich in information that supports clinical decisions. On the other hand, computer-aided diagnosis (CAD) networks for medical images have seen significant success in the medical field by using advanced deep-learning algorithms to support clinical decision-making. This paper presents a method for integrating LLMs into medical-image CAD networks. The proposed framework uses LLMs to enhance the output of multiple CAD networks, such as diagnosis networks, lesion segmentation networks, and report generation networks, by summarizing and reorganizing the information presented in natural language text format. The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems. In the future, LLM's medical knowledge can be also used to improve the performance of vision-based medical-image CAD models.},
   author = {Sheng Wang and Zihao Zhao and Xi Ouyang and Qian Wang and Dinggang Shen},
   month = {2},
   title = {ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models},
   url = {http://arxiv.org/abs/2302.07257},
   year = {2023},
}
